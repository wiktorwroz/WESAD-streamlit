{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja Emocji z ML Time Series - WESAD Dataset\n",
    "\n",
    "## Plan analizy\n",
    "\n",
    "Ten notebook wykonuje kompletnƒÖ analizƒô klasyfikacji emocji z danych WESAD u≈ºywajƒÖc **modelu time series (LSTM/GRU)** zamiast tradycyjnych modeli ML.\n",
    "\n",
    "1. **Import bibliotek** - numpy, pandas, matplotlib, seaborn, scikit-learn, imblearn, tensorflow/keras\n",
    "2. **Wczytanie danych** - CSV/PKL z WESAD, sprawdzenie rozk≈Çadu klas\n",
    "3. **Segmentacja danych** - sliding windows z ekstrakcjƒÖ cech (mean, std, min, max, range, RMS, kurtosis, skewness, RMSSD, slope, respiration rate)\n",
    "4. **Encoding i skalowanie** - LabelEncoder dla targetu, StandardScaler dla cech\n",
    "5. **Podzia≈Ç Train/Test** - **Subject-wise split** (ca≈Çe osoby do train/test, nie dzielimy okien)\n",
    "6. **Balansowanie** - SMOTE na train, weryfikacja balansu\n",
    "7. **Przygotowanie danych dla time series** - tworzenie sekwencji z okien\n",
    "8. **Trenowanie modelu time series** - LSTM/GRU\n",
    "9. **Ewaluacja** - confusion matrix, accuracy, balanced accuracy, macro F1, per-class metrics\n",
    "10. **Wnioski i raport** - analiza wynik√≥w modelu time series\n",
    "11. **Wizualizacje** - wykresy rozk≈Çadu klas, metryk, confusion matrices\n",
    "\n",
    "## ‚ö†Ô∏è WA≈ªNE: Subject-wise Split\n",
    "\n",
    "- **Ca≈Çe dane jednej osoby** trafiajƒÖ albo do train, albo do test\n",
    "- **Nigdy nie dzielimy** okien z tej samej osoby miƒôdzy train i test\n",
    "- To zapewnia **realistycznƒÖ generalizacjƒô** na nowych osobach\n",
    "\n",
    "## üÜï R√≥≈ºnice wzglƒôdem 06_klasyfikacja_emocji_smote.ipynb\n",
    "\n",
    "- U≈ºywamy **tylko jednego modelu** - ML Time Series (LSTM/GRU)\n",
    "- Dane sƒÖ przekszta≈Çcane w **sekwencje czasowe** (ka≈ºde okno to jeden timestep)\n",
    "- Model uczy siƒô **zale≈ºno≈õci temporalnych** miƒôdzy oknami\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 1: IMPORT BIBLIOTEK\n",
    "# ============================================================================\n",
    "\n",
    "# Podstawowe importy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Podstawowe biblioteki zaimportowane (numpy, pandas, matplotlib, seaborn)\")\n",
    "\n",
    "# Scikit-learn\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix, classification_report, accuracy_score,\n",
    "        balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "    )\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    print(\"‚úÖ Scikit-learn zaimportowany\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå B≈ÇƒÖd importu scikit-learn: {e}\")\n",
    "    raise\n",
    "\n",
    "# Imbalanced-learn (SMOTE)\n",
    "# Opcja: ustaw SKIP_IMBLEARN = True je≈õli imbalanced-learn powoduje crash\n",
    "# Domy≈õlnie ustawione na True, aby uniknƒÖƒá crashu - zmie≈Ñ na False je≈õli chcesz u≈ºyƒá SMOTE\n",
    "SKIP_IMBLEARN = True  # Zmie≈Ñ na False je≈õli chcesz u≈ºyƒá SMOTE (wymaga imbalanced-learn)\n",
    "\n",
    "IMBLEARN_AVAILABLE = False\n",
    "\n",
    "if not SKIP_IMBLEARN:\n",
    "    print(\"üì¶ Pr√≥ba importu imbalanced-learn...\")\n",
    "    try:\n",
    "        # Wy≈ÇƒÖcz warningi imbalanced-learn je≈õli powodujƒÖ problemy\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "        \n",
    "        IMBLEARN_AVAILABLE = True\n",
    "        print(\"‚úÖ imbalanced-learn zaimportowany (SMOTE dostƒôpny)\")\n",
    "    except ImportError as e:\n",
    "        IMBLEARN_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è imbalanced-learn niedostƒôpny - zainstaluj: pip install imbalanced-learn\")\n",
    "        print(f\"   B≈ÇƒÖd: {e}\")\n",
    "    except Exception as e:\n",
    "        IMBLEARN_AVAILABLE = False\n",
    "        print(f\"‚ö†Ô∏è B≈ÇƒÖd podczas importu imbalanced-learn: {type(e).__name__}: {e}\")\n",
    "        print(\"   Kontynuujƒô bez SMOTE...\")\n",
    "        print(\"   Je≈õli imbalanced-learn powoduje crash kernela, ustaw SKIP_IMBLEARN = True na poczƒÖtku tej sekcji\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è imbalanced-learn pominiƒôty (SKIP_IMBLEARN = True)\")\n",
    "    print(\"   SMOTE nie bƒôdzie dostƒôpny - balansowanie danych mo≈ºe nie dzia≈Çaƒá\")\n",
    "\n",
    "# Scipy dla sygna≈Ç√≥w\n",
    "# Opcja: ustaw SKIP_SCIPY = True je≈õli scipy powoduje crash\n",
    "SKIP_SCIPY = True  # Zmie≈Ñ na False je≈õli chcesz u≈ºyƒá scipy (mo≈ºe powodowaƒá crash)\n",
    "\n",
    "SCIPY_AVAILABLE = False\n",
    "\n",
    "if not SKIP_SCIPY:\n",
    "    print(\"üì¶ Pr√≥ba importu scipy...\")\n",
    "    try:\n",
    "        from scipy.signal import resample\n",
    "        from scipy import stats\n",
    "        SCIPY_AVAILABLE = True\n",
    "        print(\"‚úÖ Scipy zaimportowany\")\n",
    "    except ImportError as e:\n",
    "        SCIPY_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è Scipy niedostƒôpny - zainstaluj: pip install scipy\")\n",
    "        print(f\"   B≈ÇƒÖd: {e}\")\n",
    "        print(\"   Kontynuujƒô bez scipy - funkcja resample mo≈ºe nie dzia≈Çaƒá\")\n",
    "    except Exception as e:\n",
    "        SCIPY_AVAILABLE = False\n",
    "        print(f\"‚ö†Ô∏è B≈ÇƒÖd podczas importu scipy: {type(e).__name__}: {e}\")\n",
    "        print(\"   Kontynuujƒô bez scipy - funkcja resample mo≈ºe nie dzia≈Çaƒá\")\n",
    "        print(\"   Je≈õli scipy powoduje crash kernela, ustaw SKIP_SCIPY = True na poczƒÖtku tej sekcji\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Scipy pominiƒôty (SKIP_SCIPY = True)\")\n",
    "    print(\"   Funkcja resample nie bƒôdzie dostƒôpna - dane mogƒÖ nie byƒá resamplowane\")\n",
    "\n",
    "# TensorFlow/Keras dla modeli time series (LSTM/GRU)\n",
    "# Uwaga: Import TensorFlow mo≈ºe powodowaƒá crash kernela - obs≈Çugujemy to bezpiecznie\n",
    "TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# Opcja: ustaw SKIP_TENSORFLOW = True je≈õli TensorFlow powoduje crash\n",
    "# Domy≈õlnie ustawione na False, aby u≈ºyƒá TensorFlow - zmie≈Ñ na True je≈õli TensorFlow powoduje problemy\n",
    "SKIP_TENSORFLOW = False  # Zmie≈Ñ na True je≈õli TensorFlow powoduje crash kernela\n",
    "\n",
    "if not SKIP_TENSORFLOW:\n",
    "    print(\"üì¶ Pr√≥ba importu TensorFlow/Keras...\")\n",
    "    try:\n",
    "        # Wy≈ÇƒÖcz logi TensorFlow aby uniknƒÖƒá problem√≥w\n",
    "        import os\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Ukryj warningi\n",
    "        \n",
    "        # WY≈ÅƒÑCZ GPU - zapobiega crashowi kernela\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "        \n",
    "        print(\"   Importowanie tensorflow...\")\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Wy≈ÇƒÖcz GPU w TensorFlow\n",
    "        tf.config.set_visible_devices([], 'GPU')\n",
    "        print(\"   ‚úÖ GPU wy≈ÇƒÖczone - u≈ºywam tylko CPU\")\n",
    "        \n",
    "        print(\"   Importowanie modu≈Ç√≥w Keras...\")\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "        from tensorflow.keras.utils import to_categorical\n",
    "        \n",
    "        TENSORFLOW_AVAILABLE = True\n",
    "        print(\"‚úÖ TensorFlow/Keras dostƒôpny - modele time series bƒôdƒÖ dostƒôpne\")\n",
    "        print(f\"   Wersja TensorFlow: {tf.__version__}\")\n",
    "    except ImportError as e:\n",
    "        TENSORFLOW_AVAILABLE = true\n",
    "        print(\"‚ö†Ô∏è TensorFlow/Keras niedostƒôpny - modele time series nie bƒôdƒÖ dostƒôpne\")\n",
    "        print(\"   Zainstaluj: pip install tensorflow\")\n",
    "        print(f\"   B≈ÇƒÖd: {e}\")\n",
    "    except Exception as e:\n",
    "        TENSORFLOW_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è TensorFlow/Keras niedostƒôpny - wystƒÖpi≈Ç b≈ÇƒÖd podczas importu\")\n",
    "        print(f\"   B≈ÇƒÖd: {type(e).__name__}: {e}\")\n",
    "        print(\"   Mo≈ºesz kontynuowaƒá bez TensorFlow, ale modele time series nie bƒôdƒÖ dostƒôpne\")\n",
    "        print(\"   Je≈õli TensorFlow powoduje crash kernela, ustaw SKIP_TENSORFLOW = True na poczƒÖtku tej kom√≥rki\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TensorFlow pominiƒôty (SKIP_TENSORFLOW = True)\")\n",
    "    print(\"   Aby u≈ºyƒá TensorFlow, zmie≈Ñ SKIP_TENSORFLOW = False w tej kom√≥rce\")\n",
    "    print(\"   Modele time series nie bƒôdƒÖ dostƒôpne\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Wszystkie biblioteki zaimportowane pomy≈õlnie!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 2: WCZYTYWANIE DANYCH\n",
    "\n",
    "Wczytujemy dane WESAD z plik√≥w CSV i PKL. Sprawdzamy rozk≈Çad klas.\n",
    "\n",
    "**UWAGA:** Ten krok jest identyczny jak w `06_klasyfikacja_emocji_smote.ipynb` - u≈ºywamy tych samych funkcji wczytywania danych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce importy!\n",
      "   B≈ÇƒÖd: name 'Path' is not defined\n",
      "   Najpierw uruchom KROK 1 (kom√≥rka z importami bibliotek)!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "BrakujƒÖce importy - uruchom najpierw KROK 1: name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Sprawd≈∫ czy podstawowe importy sƒÖ dostƒôpne\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     _ \u001b[38;5;241m=\u001b[39m Path\n\u001b[1;32m      9\u001b[0m     _ \u001b[38;5;241m=\u001b[39m pd\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   B≈ÇƒÖd: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Najpierw uruchom KROK 1 (kom√≥rka z importami bibliotek)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrakujƒÖce importy - uruchom najpierw KROK 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ≈öcie≈ºki\u001b[39;00m\n\u001b[1;32m     19\u001b[0m RAW_ROOT \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/turfian/Downloads/archive (4)/WESAD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: BrakujƒÖce importy - uruchom najpierw KROK 1: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 2: WCZYTYWANIE DANYCH\n",
    "# ============================================================================\n",
    "\n",
    "# Sprawd≈∫ czy importy zosta≈Çy wykonane (KROK 1)\n",
    "try:\n",
    "    # Sprawd≈∫ czy podstawowe importy sƒÖ dostƒôpne\n",
    "    _ = Path\n",
    "    _ = pd\n",
    "    _ = np\n",
    "    _ = pickle\n",
    "except NameError as e:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce importy!\")\n",
    "    print(f\"   B≈ÇƒÖd: {e}\")\n",
    "    print(\"   Najpierw uruchom KROK 1 (kom√≥rka z importami bibliotek)!\")\n",
    "    raise NameError(f\"BrakujƒÖce importy - uruchom najpierw KROK 1: {e}\")\n",
    "\n",
    "# ≈öcie≈ºki\n",
    "RAW_ROOT = Path(\"/Users/turfian/Downloads/archive (4)/WESAD\")\n",
    "PROJECT_ROOT = Path(\"/Users/turfian/Downloads/archive (4)/WESAD/wesad-prep\")\n",
    "\n",
    "# Parametry\n",
    "TARGET_FS = 32.0\n",
    "MAX_DURATION = pd.Timedelta(minutes=40)\n",
    "DEFAULT_SUBJECTS = [\"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\"]\n",
    "\n",
    "# Mapowanie faz do klas (zgodnie z rzeczywistymi nazwami faz w WESAD)\n",
    "PHASE_TO_CLASS = {\n",
    "    \"Base\": \"baseline\",\n",
    "    \"Medi 1\": \"baseline\",\n",
    "    \"Medi 2\": \"baseline\",\n",
    "    \"TSST\": \"stress\",\n",
    "    \"sRead\": \"stress\",\n",
    "    \"fRead\": \"stress\",\n",
    "    \"Fun\": \"amusement\",\n",
    "    # Alternatywne nazwy (na wypadek r√≥≈ºnic w plikach)\n",
    "    \"Stress\": \"stress\",\n",
    "    \"Amusement\": \"amusement\",\n",
    "    \"Meditation\": \"baseline\",\n",
    "}\n",
    "\n",
    "# Funkcje pomocnicze do parsowania\n",
    "def build_time_index(length: int, start_ts: float, fs: float) -> pd.Series:\n",
    "    \"\"\"Buduje indeks czasowy dla sygna≈Çu\"\"\"\n",
    "    start = pd.to_datetime(start_ts, unit=\"s\", utc=True)\n",
    "    offsets = pd.to_timedelta(np.arange(length) / fs, unit=\"s\")\n",
    "    return start + offsets\n",
    "\n",
    "def load_sensor_for_subject(subject_path: Path, sensor_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Wczytuje dane z sensora (CSV)\"\"\"\n",
    "    file_path = subject_path / f\"{sensor_name}.csv\"\n",
    "    if not file_path.exists():\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    header = pd.read_csv(file_path, nrows=2, header=None)\n",
    "    start_ts = float(header.iloc[0, 0])\n",
    "    fs = float(header.iloc[1, 0])\n",
    "    \n",
    "    column_names = {\n",
    "        \"ACC\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "        \"EDA\": [\"eda\"],\n",
    "        \"BVP\": [\"bvp\"],\n",
    "        \"TEMP\": [\"temp\"],\n",
    "        \"HR\": [\"hr\"],\n",
    "    }.get(sensor_name, [sensor_name.lower()])\n",
    "    \n",
    "    data = pd.read_csv(file_path, skiprows=2, header=None, names=column_names)\n",
    "    data.insert(0, \"timestamp\", build_time_index(len(data), start_ts, fs))\n",
    "    data.attrs.update({\"start_ts\": start_ts, \"fs\": fs})\n",
    "    return data\n",
    "\n",
    "def load_wesad_pickle(subject: str, raw_root: Path = RAW_ROOT) -> dict:\n",
    "    \"\"\"Wczytuje dane z pliku PKL\"\"\"\n",
    "    pkl_path = raw_root / subject / f\"{subject}.pkl\"\n",
    "    if not pkl_path.exists():\n",
    "        raise FileNotFoundError(f\"Brak pliku {pkl_path}\")\n",
    "    with pkl_path.open(\"rb\") as handle:\n",
    "        return pickle.load(handle, encoding=\"latin1\")\n",
    "\n",
    "def load_tags_for_subject(subject_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Wczytuje tagi (etykiety) dla subjecta\"\"\"\n",
    "    path = subject_path / \"tags.csv\"\n",
    "    if not path.exists() or path.stat().st_size == 0:\n",
    "        return pd.DataFrame(columns=[\"timestamp\", \"tag\"])\n",
    "    tags = pd.read_csv(path, header=None, names=[\"timestamp\"])\n",
    "    tags[\"timestamp\"] = pd.to_datetime(tags[\"timestamp\"], unit=\"s\", utc=True)\n",
    "    tags[\"tag\"] = 1\n",
    "    return tags\n",
    "\n",
    "def build_phase_protocol_for_subject(subject: str, session_start: pd.Timestamp, raw_root: Path = RAW_ROOT) -> pd.DataFrame:\n",
    "    \"\"\"Buduje protok√≥≈Ç faz dla subjecta z pliku *_quest.csv\"\"\"\n",
    "    quest_path = raw_root / subject / f\"{subject}_quest.csv\"\n",
    "    if not quest_path.exists():\n",
    "        return pd.DataFrame(columns=[\"phase\", \"start\", \"end\", \"duration_s\"])\n",
    "    \n",
    "    lines = [line.strip() for line in quest_path.read_text().splitlines() if line.strip()]\n",
    "    \n",
    "    def _extract_values(lines, prefix):\n",
    "        for line in lines:\n",
    "            if line.startswith(prefix):\n",
    "                return [token for token in line.split(\";\")[1:] if token]\n",
    "        return []\n",
    "    \n",
    "    names = _extract_values(lines, \"# ORDER\")\n",
    "    starts = _extract_values(lines, \"# START\")\n",
    "    ends = _extract_values(lines, \"# END\")\n",
    "    \n",
    "    phases = []\n",
    "    limit = min(len(names), len(starts), len(ends))\n",
    "    for idx in range(limit):\n",
    "        try:\n",
    "            start_sec = float(starts[idx])\n",
    "            end_sec = float(ends[idx])\n",
    "            phase_name = names[idx].strip()\n",
    "            phases.append({\n",
    "                \"phase\": phase_name,\n",
    "                \"start\": session_start + pd.to_timedelta(start_sec, unit=\"s\"),\n",
    "                \"end\": session_start + pd.to_timedelta(end_sec, unit=\"s\"),\n",
    "                \"duration_s\": end_sec - start_sec\n",
    "            })\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(phases)\n",
    "\n",
    "def assign_phase_labels(timestamps: pd.Series, phases: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Przypisuje etykiety faz do timestamp√≥w\"\"\"\n",
    "    if phases.empty:\n",
    "        return pd.Series([\"unknown\"] * len(timestamps), index=timestamps.index)\n",
    "    intervals = pd.IntervalIndex.from_arrays(phases[\"start\"], phases[\"end\"], closed=\"left\")\n",
    "    labels = phases[\"phase\"].to_list()\n",
    "    idx = intervals.get_indexer(timestamps)\n",
    "    label_array = np.array(labels, dtype=object)\n",
    "    mapped = np.where(idx >= 0, label_array[idx], \"unknown\")\n",
    "    return pd.Series(mapped, index=timestamps.index)\n",
    "\n",
    "def resample_signal(array, src_fs: float, target_fs: float, target_len: int) -> np.ndarray:\n",
    "    \"\"\"Resampluje sygna≈Ç do docelowej czƒôstotliwo≈õci\"\"\"\n",
    "    # Sprawd≈∫ czy scipy jest dostƒôpny\n",
    "    try:\n",
    "        SCIPY_AVAILABLE\n",
    "    except NameError:\n",
    "        SCIPY_AVAILABLE = False\n",
    "    \n",
    "    if not SCIPY_AVAILABLE:\n",
    "        # Je≈õli scipy nie jest dostƒôpny, u≈ºyj prostego interpolacji numpy\n",
    "        if array.ndim == 1:\n",
    "            array = array[:, None]\n",
    "        # U≈ºyj numpy.interp jako alternatywy\n",
    "        original_indices = np.linspace(0, len(array) - 1, len(array))\n",
    "        target_indices = np.linspace(0, len(array) - 1, target_len)\n",
    "        resampled = np.zeros((target_len, array.shape[1]))\n",
    "        for i in range(array.shape[1]):\n",
    "            resampled[:, i] = np.interp(target_indices, original_indices, array[:, i])\n",
    "        return resampled.flatten() if resampled.shape[1] == 1 else resampled\n",
    "    \n",
    "    if array.ndim == 1:\n",
    "        array = array[:, None]\n",
    "    expected_len = int(src_fs * MAX_DURATION.total_seconds())\n",
    "    trimmed = array[:expected_len]\n",
    "    if len(trimmed) == 0:\n",
    "        return np.full((target_len, array.shape[1]), np.nan)\n",
    "    return resample(trimmed, target_len, axis=0)\n",
    "\n",
    "# Wczytanie danych dla wszystkich subject√≥w\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 2: WCZYTYWANIE DANYCH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_subjects_data = []\n",
    "\n",
    "for subject in DEFAULT_SUBJECTS:\n",
    "    print(f\"\\nüìÇ Wczytujƒô dane dla {subject}...\")\n",
    "    subject_path = RAW_ROOT / subject / f\"{subject}_E4_Data\"\n",
    "    \n",
    "    if not subject_path.exists():\n",
    "        print(f\"  ‚ö†Ô∏è Brak folderu {subject_path} - pomijam\")\n",
    "        continue\n",
    "    \n",
    "    # Wczytaj sygna≈Çy z nadgarstka (CSV)\n",
    "    wrist_data = {}\n",
    "    for sensor in [\"ACC\", \"EDA\", \"BVP\", \"TEMP\"]:\n",
    "        sensor_df = load_sensor_for_subject(subject_path, sensor)\n",
    "        if not sensor_df.empty:\n",
    "            wrist_data[sensor.lower()] = sensor_df\n",
    "    \n",
    "    # Sprawd≈∫ czy mamy dane\n",
    "    if not wrist_data:\n",
    "        print(f\"  ‚ö†Ô∏è Brak danych z nadgarstka - pomijam\")\n",
    "        continue\n",
    "    \n",
    "    # U≈ºyj timestamp z pierwszego sensora jako session_start\n",
    "    first_sensor = list(wrist_data.values())[0]\n",
    "    if len(first_sensor) == 0:\n",
    "        print(f\"  ‚ö†Ô∏è Pusty sensor - pomijam\")\n",
    "        continue\n",
    "    \n",
    "    session_start = first_sensor[\"timestamp\"].iloc[0]\n",
    "    \n",
    "    # Wczytaj protok√≥≈Ç faz\n",
    "    phases = build_phase_protocol_for_subject(subject, session_start)\n",
    "    \n",
    "    # Po≈ÇƒÖcz dane nadgarstka\n",
    "    if wrist_data:\n",
    "        # U≈ºyj BVP jako referencji czasowej\n",
    "        if \"bvp\" in wrist_data:\n",
    "            base_df = wrist_data[\"bvp\"][[\"timestamp\"]].copy()\n",
    "            for sensor_name, sensor_df in wrist_data.items():\n",
    "                if sensor_name != \"bvp\":\n",
    "                    # Resample do czƒôstotliwo≈õci BVP\n",
    "                    merged = pd.merge_asof(\n",
    "                        base_df.sort_values(\"timestamp\"),\n",
    "                        sensor_df[[\"timestamp\"] + [col for col in sensor_df.columns if col != \"timestamp\"]].sort_values(\"timestamp\"),\n",
    "                        on=\"timestamp\",\n",
    "                        direction=\"nearest\",\n",
    "                        tolerance=pd.Timedelta(seconds=1)\n",
    "                    )\n",
    "                    for col in sensor_df.columns:\n",
    "                        if col != \"timestamp\":\n",
    "                            base_df[col] = merged[col].values\n",
    "            \n",
    "            # Dodaj pozosta≈Çe kolumny z BVP\n",
    "            for col in wrist_data[\"bvp\"].columns:\n",
    "                if col != \"timestamp\" and col not in base_df.columns:\n",
    "                    base_df[col] = wrist_data[\"bvp\"][col].values\n",
    "            \n",
    "            # Resample do docelowej czƒôstotliwo≈õci\n",
    "            target_len = int(MAX_DURATION.total_seconds() * TARGET_FS)\n",
    "            timestamps = session_start + pd.to_timedelta(np.arange(target_len) / TARGET_FS, unit=\"s\")\n",
    "            \n",
    "            # Resample ka≈ºdej kolumny\n",
    "            resampled_data = {}\n",
    "            for col in base_df.columns:\n",
    "                if col != \"timestamp\":\n",
    "                    original_values = base_df[col].values\n",
    "                    if len(original_values) > 0:\n",
    "                        resampled = resample_signal(original_values, wrist_data[\"bvp\"].attrs[\"fs\"], TARGET_FS, target_len)\n",
    "                        resampled_data[col] = resampled.flatten() if resampled.ndim > 1 else resampled\n",
    "                    else:\n",
    "                        resampled_data[col] = np.full(target_len, np.nan)\n",
    "            \n",
    "            # Stw√≥rz DataFrame\n",
    "            subject_df = pd.DataFrame(resampled_data)\n",
    "            subject_df.insert(0, \"timestamp\", timestamps)\n",
    "            \n",
    "            # Dodaj etykiety faz\n",
    "            subject_df[\"phase\"] = assign_phase_labels(subject_df[\"timestamp\"], phases)\n",
    "            subject_df[\"label\"] = subject_df[\"phase\"].map(PHASE_TO_CLASS).fillna(\"unknown\")\n",
    "            subject_df[\"subject\"] = subject\n",
    "            \n",
    "            # Sprawd≈∫ rozk≈Çad faz i klas dla tego subjecta\n",
    "            phase_dist = subject_df[\"phase\"].value_counts()\n",
    "            label_dist = subject_df[\"label\"].value_counts()\n",
    "            print(f\"    Fazy: {dict(phase_dist)}\")\n",
    "            print(f\"    Klasy: {dict(label_dist)}\")\n",
    "            \n",
    "            all_subjects_data.append(subject_df)\n",
    "            print(f\"  ‚úÖ Wczytano {len(subject_df)} pr√≥bek\")\n",
    "\n",
    "# Po≈ÇƒÖcz wszystkie dane\n",
    "if all_subjects_data:\n",
    "    full_data = pd.concat(all_subjects_data, ignore_index=True)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PODSUMOWANIE WCZYTYWANIA DANYCH\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"‚úÖ Wczytano dane dla {len(all_subjects_data)} subject√≥w\")\n",
    "    print(f\"   ≈ÅƒÖczna liczba pr√≥bek: {len(full_data)}\")\n",
    "    \n",
    "    print(f\"\\nüìä SZCZEG√ì≈ÅOWY ROZK≈ÅAD KLAS PER SUBJECT:\")\n",
    "    print(\"-\" * 80)\n",
    "    for subject in full_data[\"subject\"].unique():\n",
    "        subject_data = full_data[full_data[\"subject\"] == subject]\n",
    "        print(f\"\\n  {subject}:\")\n",
    "        label_dist = subject_data[\"label\"].value_counts()\n",
    "        for label in label_dist.index:\n",
    "            count = label_dist[label]\n",
    "            pct = (count / len(subject_data) * 100) if len(subject_data) > 0 else 0\n",
    "            print(f\"    {label:12s}: {count:6d} pr√≥bek ({pct:6.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìä ROZK≈ÅAD FAZ PER SUBJECT:\")\n",
    "    print(\"-\" * 80)\n",
    "    for subject in full_data[\"subject\"].unique():\n",
    "        subject_data = full_data[full_data[\"subject\"] == subject]\n",
    "        print(f\"\\n  {subject}:\")\n",
    "        phase_dist = subject_data[\"phase\"].value_counts()\n",
    "        for phase in phase_dist.index:\n",
    "            count = phase_dist[phase]\n",
    "            pct = (count / len(subject_data) * 100) if len(subject_data) > 0 else 0\n",
    "            print(f\"    {phase:15s}: {count:6d} pr√≥bek ({pct:6.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìä GLOBALNY ROZK≈ÅAD KLAS (wszystkie subjecty):\")\n",
    "    print(\"-\" * 80)\n",
    "    class_dist = full_data[\"label\"].value_counts()\n",
    "    for label in class_dist.index:\n",
    "        count = class_dist[label]\n",
    "        pct = (count / len(full_data) * 100) if len(full_data) > 0 else 0\n",
    "        print(f\"   {label:12s}: {count:6d} pr√≥bek ({pct:6.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìä GLOBALNY ROZK≈ÅAD FAZ (wszystkie subjecty):\")\n",
    "    print(\"-\" * 80)\n",
    "    phase_dist = full_data[\"phase\"].value_counts()\n",
    "    for phase in phase_dist.index:\n",
    "        count = phase_dist[phase]\n",
    "        pct = (count / len(full_data) * 100) if len(full_data) > 0 else 0\n",
    "        print(f\"   {phase:15s}: {count:6d} pr√≥bek ({pct:6.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìä KOLUMNY W DANYCH:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"   Kolumny sygna≈Ç√≥w: {[col for col in full_data.columns if col not in ['timestamp', 'phase', 'label', 'subject']]}\")\n",
    "    print(f\"   Liczba kolumn sygna≈Ç√≥w: {len([col for col in full_data.columns if col not in ['timestamp', 'phase', 'label', 'subject']])}\")\n",
    "    \n",
    "    print(f\"\\nüìä KILKA PIERWSZYCH WIERSZY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(full_data.head(10))\n",
    "    \n",
    "    print(f\"\\nüìä STATYSTYKI CZASOWE:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"   Najwcze≈õniejszy timestamp: {full_data['timestamp'].min()}\")\n",
    "    print(f\"   Najp√≥≈∫niejszy timestamp: {full_data['timestamp'].max()}\")\n",
    "    print(f\"   Czas trwania: {(full_data['timestamp'].max() - full_data['timestamp'].min()).total_seconds() / 60:.2f} minut\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ WCZYTYWANIE DANYCH ZAKO≈ÉCZONE POMY≈öLNIE!\")\n",
    "else:\n",
    "    print(\"‚ùå Nie wczytano ≈ºadnych danych!\")\n",
    "    raise ValueError(\"Brak danych do analizy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 3: SEGMENTACJA DANYCH (SLIDING WINDOWS)\n",
    "\n",
    "Tworzymy okna czasowe i wyciƒÖgamy statystyczne cechy z ka≈ºdego okna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: full_data, TARGET_FS\n",
      "   Najpierw uruchom KROK 2!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "BrakujƒÖce zmienne: full_data, TARGET_FS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Najpierw uruchom KROK 2!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Parametry segmentacji\u001b[39;00m\n\u001b[1;32m     15\u001b[0m WINDOW_SIZE_SECONDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Rozmiar okna w sekundach\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: BrakujƒÖce zmienne: full_data, TARGET_FS"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 3: SEGMENTACJA DANYCH (SLIDING WINDOWS)\n",
    "# ============================================================================\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá zmiennych\n",
    "required_vars = ['full_data', 'TARGET_FS']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "    print(\"   Najpierw uruchom KROK 2!\")\n",
    "    raise NameError(f\"BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "\n",
    "# Parametry segmentacji\n",
    "WINDOW_SIZE_SECONDS = 5  # Rozmiar okna w sekundach\n",
    "STEP_SIZE_SECONDS = 2.5  # Krok (50% overlap)\n",
    "WINDOW_SIZE = int(WINDOW_SIZE_SECONDS * TARGET_FS)  # Rozmiar okna w pr√≥bkach\n",
    "STEP_SIZE = int(STEP_SIZE_SECONDS * TARGET_FS)  # Krok w pr√≥bkach\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 3: SEGMENTACJA DANYCH (SLIDING WINDOWS)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Rozmiar okna: {WINDOW_SIZE_SECONDS} sekund ({WINDOW_SIZE} pr√≥bek)\")\n",
    "print(f\"  Krok: {STEP_SIZE_SECONDS} sekund ({STEP_SIZE} pr√≥bek)\")\n",
    "print(f\"  Overlap: {(1 - STEP_SIZE_SECONDS/WINDOW_SIZE_SECONDS)*100:.1f}%\")\n",
    "\n",
    "# Funkcje do ekstrakcji cech\n",
    "def compute_rms(signal):\n",
    "    \"\"\"Oblicza RMS (Root Mean Square)\"\"\"\n",
    "    return np.sqrt(np.mean(signal**2))\n",
    "\n",
    "def compute_kurtosis(signal):\n",
    "    \"\"\"Oblicza kurtozƒô\"\"\"\n",
    "    if len(signal) < 4:\n",
    "        return 0.0\n",
    "    # Sprawd≈∫ czy scipy jest dostƒôpny\n",
    "    try:\n",
    "        SCIPY_AVAILABLE\n",
    "    except NameError:\n",
    "        SCIPY_AVAILABLE = False\n",
    "    \n",
    "    if SCIPY_AVAILABLE:\n",
    "        try:\n",
    "            return stats.kurtosis(signal, nan_policy='omit')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Oblicz kurtozƒô rƒôcznie (bez scipy)\n",
    "    signal_clean = signal[~np.isnan(signal)]\n",
    "    if len(signal_clean) < 4:\n",
    "        return 0.0\n",
    "    mean = np.mean(signal_clean)\n",
    "    std = np.std(signal_clean)\n",
    "    if std == 0:\n",
    "        return 0.0\n",
    "    n = len(signal_clean)\n",
    "    kurt = np.mean(((signal_clean - mean) / std) ** 4) - 3\n",
    "    return kurt\n",
    "\n",
    "def compute_skewness(signal):\n",
    "    \"\"\"Oblicza sko≈õno≈õƒá\"\"\"\n",
    "    if len(signal) < 3:\n",
    "        return 0.0\n",
    "    # Sprawd≈∫ czy scipy jest dostƒôpny\n",
    "    try:\n",
    "        SCIPY_AVAILABLE\n",
    "    except NameError:\n",
    "        SCIPY_AVAILABLE = False\n",
    "    \n",
    "    if SCIPY_AVAILABLE:\n",
    "        try:\n",
    "            return stats.skew(signal, nan_policy='omit')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Oblicz sko≈õno≈õƒá rƒôcznie (bez scipy)\n",
    "    signal_clean = signal[~np.isnan(signal)]\n",
    "    if len(signal_clean) < 3:\n",
    "        return 0.0\n",
    "    mean = np.mean(signal_clean)\n",
    "    std = np.std(signal_clean)\n",
    "    if std == 0:\n",
    "        return 0.0\n",
    "    n = len(signal_clean)\n",
    "    skew = np.mean(((signal_clean - mean) / std) ** 3)\n",
    "    return skew\n",
    "\n",
    "def compute_rmssd(signal):\n",
    "    \"\"\"Oblicza RMSSD (Root Mean Square of Successive Differences) - dla HRV\"\"\"\n",
    "    if len(signal) < 2:\n",
    "        return 0.0\n",
    "    diff = np.diff(signal)\n",
    "    return np.sqrt(np.mean(diff**2))\n",
    "\n",
    "def compute_slope(signal):\n",
    "    \"\"\"Oblicza nachylenie (slope) - trend liniowy\"\"\"\n",
    "    if len(signal) < 2:\n",
    "        return 0.0\n",
    "    x = np.arange(len(signal))\n",
    "    coeffs = np.polyfit(x, signal, 1)\n",
    "    return coeffs[0]\n",
    "\n",
    "def compute_respiration_rate(signal, fs=TARGET_FS):\n",
    "    \"\"\"Oblicza tempo oddechu (dla sygna≈Çu respiracji)\"\"\"\n",
    "    if len(signal) < int(fs * 2):  # Minimum 2 sekundy\n",
    "        return 0.0\n",
    "    \n",
    "    # Sprawd≈∫ czy scipy jest dostƒôpny\n",
    "    try:\n",
    "        SCIPY_AVAILABLE\n",
    "    except NameError:\n",
    "        SCIPY_AVAILABLE = False\n",
    "    \n",
    "    # Znajd≈∫ peaki\n",
    "    if SCIPY_AVAILABLE:\n",
    "        try:\n",
    "            from scipy.signal import find_peaks\n",
    "            peaks, _ = find_peaks(signal, distance=int(fs * 0.5))  # Minimum 0.5s miƒôdzy peakami\n",
    "        except:\n",
    "            # Fallback do prostego algorytmu\n",
    "            peaks = []\n",
    "            threshold = np.mean(signal) + 0.5 * np.std(signal)\n",
    "            min_distance = int(fs * 0.5)\n",
    "            for i in range(min_distance, len(signal) - min_distance):\n",
    "                if signal[i] > threshold and signal[i] == np.max(signal[i-min_distance:i+min_distance+1]):\n",
    "                    peaks.append(i)\n",
    "            peaks = np.array(peaks)\n",
    "    else:\n",
    "        # Prosty algorytm znajdowania peak√≥w bez scipy\n",
    "        peaks = []\n",
    "        threshold = np.mean(signal) + 0.5 * np.std(signal)\n",
    "        min_distance = int(fs * 0.5)\n",
    "        for i in range(min_distance, len(signal) - min_distance):\n",
    "            if signal[i] > threshold and signal[i] == np.max(signal[i-min_distance:i+min_distance+1]):\n",
    "                peaks.append(i)\n",
    "        peaks = np.array(peaks)\n",
    "    \n",
    "    if len(peaks) < 2:\n",
    "        return 0.0\n",
    "    # Oblicz ≈õredni czas miƒôdzy peakami\n",
    "    peak_intervals = np.diff(peaks) / fs\n",
    "    avg_interval = np.mean(peak_intervals)\n",
    "    if avg_interval > 0:\n",
    "        return 60.0 / avg_interval  # Oddechy na minutƒô\n",
    "    return 0.0\n",
    "\n",
    "def extract_features_from_window(window_data):\n",
    "    \"\"\"WyciƒÖga cechy statystyczne z okna\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Kolumny sygna≈Ç√≥w (pomijamy timestamp, phase, label, subject)\n",
    "    signal_cols = [col for col in window_data.columns \n",
    "                   if col not in [\"timestamp\", \"phase\", \"label\", \"subject\"]]\n",
    "    \n",
    "    for col in signal_cols:\n",
    "        signal = window_data[col].values\n",
    "        signal_clean = signal[~np.isnan(signal)]\n",
    "        \n",
    "        if len(signal_clean) == 0:\n",
    "            # Je≈õli wszystkie warto≈õci sƒÖ NaN, ustaw wszystkie cechy na 0\n",
    "            features[f\"{col}_mean\"] = 0.0\n",
    "            features[f\"{col}_std\"] = 0.0\n",
    "            features[f\"{col}_min\"] = 0.0\n",
    "            features[f\"{col}_max\"] = 0.0\n",
    "            features[f\"{col}_range\"] = 0.0\n",
    "            features[f\"{col}_rms\"] = 0.0\n",
    "            features[f\"{col}_kurtosis\"] = 0.0\n",
    "            features[f\"{col}_skewness\"] = 0.0\n",
    "            features[f\"{col}_rmssd\"] = 0.0\n",
    "            features[f\"{col}_slope\"] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # Podstawowe statystyki\n",
    "        features[f\"{col}_mean\"] = np.mean(signal_clean)\n",
    "        features[f\"{col}_std\"] = np.std(signal_clean) if len(signal_clean) > 1 else 0.0\n",
    "        features[f\"{col}_min\"] = np.min(signal_clean)\n",
    "        features[f\"{col}_max\"] = np.max(signal_clean)\n",
    "        features[f\"{col}_range\"] = features[f\"{col}_max\"] - features[f\"{col}_min\"]\n",
    "        \n",
    "        # Zaawansowane cechy\n",
    "        features[f\"{col}_rms\"] = compute_rms(signal_clean)\n",
    "        features[f\"{col}_kurtosis\"] = compute_kurtosis(signal_clean)\n",
    "        features[f\"{col}_skewness\"] = compute_skewness(signal_clean)\n",
    "        features[f\"{col}_rmssd\"] = compute_rmssd(signal_clean)\n",
    "        features[f\"{col}_slope\"] = compute_slope(signal_clean)\n",
    "        \n",
    "        # Tempo oddechu (tylko dla kolumn zwiƒÖzanych z oddechem)\n",
    "        if \"resp\" in col.lower() or \"breath\" in col.lower():\n",
    "            features[f\"{col}_respiration_rate\"] = compute_respiration_rate(signal_clean)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Segmentacja sliding window\n",
    "print(f\"\\nüîß Wykonujƒô segmentacjƒô sliding window...\")\n",
    "\n",
    "segmented_data = []\n",
    "groups = []  # Dla subject-wise split\n",
    "\n",
    "for subject in full_data[\"subject\"].unique():\n",
    "    subject_data = full_data[full_data[\"subject\"] == subject].copy()\n",
    "    subject_data = subject_data.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    # Segmentacja\n",
    "    n_samples = len(subject_data)\n",
    "    for start_idx in range(0, n_samples - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        end_idx = start_idx + WINDOW_SIZE\n",
    "        window = subject_data.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # WyciƒÖgnij cechy\n",
    "        features = extract_features_from_window(window)\n",
    "        \n",
    "        # Etykieta okna (mode z okna)\n",
    "        label_counts = window[\"label\"].value_counts()\n",
    "        window_label = label_counts.index[0] if len(label_counts) > 0 else \"unknown\"\n",
    "        \n",
    "        # Dodaj metadane\n",
    "        features[\"label\"] = window_label\n",
    "        features[\"subject\"] = subject\n",
    "        features[\"window_start\"] = start_idx\n",
    "        features[\"window_end\"] = end_idx\n",
    "        \n",
    "        segmented_data.append(features)\n",
    "        groups.append(subject)\n",
    "\n",
    "# Stw√≥rz DataFrame z segmentowanych danych\n",
    "segmented_df = pd.DataFrame(segmented_data)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PODSUMOWANIE SEGMENTACJI\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Segmentacja zako≈Ñczona!\")\n",
    "print(f\"   Liczba okien: {len(segmented_df)}\")\n",
    "print(f\"   Liczba cech: {len([col for col in segmented_df.columns if col not in ['label', 'subject', 'window_start', 'window_end']])}\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas przed usuniƒôciem \"unknown\"\n",
    "print(f\"\\nüìä ROZK≈ÅAD KLAS PRZED USUNIƒòCIEM 'unknown':\")\n",
    "print(\"-\" * 80)\n",
    "class_dist_before_clean = segmented_df[\"label\"].value_counts()\n",
    "for label in class_dist_before_clean.index:\n",
    "    count = class_dist_before_clean[label]\n",
    "    pct = (count / len(segmented_df) * 100) if len(segmented_df) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "# Usu≈Ñ okna z etykietƒÖ \"unknown\"\n",
    "segmented_df = segmented_df[segmented_df[\"label\"] != \"unknown\"].copy()\n",
    "print(f\"\\nüìä Liczba okien po usuniƒôciu 'unknown': {len(segmented_df)}\")\n",
    "print(f\"   Usuniƒôto: {len(segmented_data) - len(segmented_df)} okien z etykietƒÖ 'unknown'\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas przed agregacjƒÖ\n",
    "print(f\"\\nüìä Rozk≈Çad klas PRZED agregacjƒÖ:\")\n",
    "class_dist_before_agg = segmented_df[\"label\"].value_counts()\n",
    "print(class_dist_before_agg)\n",
    "\n",
    "# ‚ö†Ô∏è WA≈ªNE: Agregacja klas (amusement + stress ‚Üí emotion)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AGREGACJA KLAS: amusement + stress ‚Üí emotion\")\n",
    "print(f\"{'='*80}\")\n",
    "segmented_df[\"label\"] = segmented_df[\"label\"].replace({\n",
    "    \"amusement\": \"emotion\",\n",
    "    \"stress\": \"emotion\"\n",
    "})\n",
    "print(f\"   ‚úÖ Agregacja wykonana: amusement + stress ‚Üí emotion\")\n",
    "\n",
    "# U≈ºyj kolumny 'subject' z segmented_df jako groups (najprostsze i najbardziej niezawodne)\n",
    "if 'subject' in segmented_df.columns:\n",
    "    groups = segmented_df['subject'].tolist()\n",
    "    print(f\"   ‚úÖ Utworzono groups z kolumny 'subject': {len(groups)} element√≥w\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è OSTRZE≈ªENIE: Brak kolumny 'subject' w segmented_df!\")\n",
    "    raise ValueError(\"Brak kolumny 'subject' w segmented_df - nie mo≈ºna utworzyƒá groups\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas po agregacji\n",
    "print(f\"\\nüìä ROZK≈ÅAD KLAS PO AGREGACJI:\")\n",
    "print(\"-\" * 80)\n",
    "class_dist_seg = segmented_df[\"label\"].value_counts()\n",
    "for label in class_dist_seg.index:\n",
    "    count = class_dist_seg[label]\n",
    "    pct = (count / len(segmented_df) * 100) if len(segmented_df) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas per subject po agregacji\n",
    "print(f\"\\nüìä ROZK≈ÅAD KLAS PER SUBJECT PO AGREGACJI:\")\n",
    "print(\"-\" * 80)\n",
    "for subject in segmented_df[\"subject\"].unique():\n",
    "    subject_data = segmented_df[segmented_df[\"subject\"] == subject]\n",
    "    print(f\"\\n  {subject}:\")\n",
    "    label_dist = subject_data[\"label\"].value_counts()\n",
    "    for label in label_dist.index:\n",
    "        count = label_dist[label]\n",
    "        pct = (count / len(subject_data) * 100) if len(subject_data) > 0 else 0\n",
    "        print(f\"    {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "# Weryfikacja: czy mamy obie klasy (baseline i emotion)\n",
    "unique_labels = segmented_df[\"label\"].unique()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"WERYFIKACJA KLAS PO SEGMENTACJI I AGREGACJI\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"   Unikalne klasy: {unique_labels}\")\n",
    "print(f\"   Liczba klas: {len(unique_labels)}\")\n",
    "\n",
    "if len(unique_labels) < 2:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: Tylko {len(unique_labels)} klas po segmentacji!\")\n",
    "    print(f\"   Musimy mieƒá co najmniej 2 klasy (baseline i emotion) dla SMOTE!\")\n",
    "    print(f\"   Sprawd≈∫ wczytywanie faz i mapowanie PHASE_TO_CLASS.\")\n",
    "    print(f\"\\nüìä DIAGNOSTYKA:\")\n",
    "    print(f\"   - Sprawd≈∫ czy fazy sƒÖ poprawnie wczytywane z plik√≥w *_quest.csv\")\n",
    "    print(f\"   - Sprawd≈∫ czy mapowanie PHASE_TO_CLASS jest poprawne\")\n",
    "    print(f\"   - Sprawd≈∫ czy assign_phase_labels dzia≈Ça poprawnie\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Mamy {len(unique_labels)} klas - OK dla SMOTE\")\n",
    "    print(f\"   ‚úÖ Klasy: {', '.join(unique_labels)}\")\n",
    "    \n",
    "    # Sprawd≈∫ balance ratio przed SMOTE\n",
    "    if len(unique_labels) == 2:\n",
    "        counts = [class_dist_seg[label] for label in unique_labels]\n",
    "        balance_ratio = min(counts) / max(counts) if max(counts) > 0 else 0\n",
    "        print(f\"   üìä Balance ratio (przed SMOTE): {balance_ratio:.4f}\")\n",
    "        if balance_ratio < 0.5:\n",
    "            print(f\"   ‚ö†Ô∏è OSTRZE≈ªENIE: Silna nier√≥wnowaga klas (balance ratio < 0.5)\")\n",
    "            print(f\"      SMOTE bƒôdzie musia≈Ç wygenerowaƒá du≈ºo syntetycznych pr√≥bek\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 4: ENCODING I SKALOWANIE CECH\n",
    "\n",
    "Kodujemy etykiety i skalujemy cechy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: segmented_df\n",
      "   Najpierw uruchom KROK 3!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "BrakujƒÖce zmienne: segmented_df",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Najpierw uruchom KROK 3!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKROK 4: ENCODING I SKALOWANIE CECH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: BrakujƒÖce zmienne: segmented_df"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 4: ENCODING I SKALOWANIE CECH\n",
    "# ============================================================================\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá zmiennych\n",
    "required_vars = ['segmented_df']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "    print(\"   Najpierw uruchom KROK 3!\")\n",
    "    raise NameError(f\"BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 4: ENCODING I SKALOWANIE CECH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Przygotuj dane\n",
    "feature_cols = [col for col in segmented_df.columns \n",
    "                if col not in [\"label\", \"subject\", \"window_start\", \"window_end\"]]\n",
    "X = segmented_df[feature_cols].copy()\n",
    "y = segmented_df[\"label\"].copy()\n",
    "\n",
    "# Usu≈Ñ kolumny z samymi NaN\n",
    "X = X.dropna(axis=1, how='all')\n",
    "\n",
    "# Wype≈Çnij pozosta≈Çe NaN zerami\n",
    "X = X.fillna(0.0)\n",
    "\n",
    "print(f\"\\nüìä Kszta≈Çt danych:\")\n",
    "print(f\"   X: {X.shape}\")\n",
    "print(f\"   y: {len(y)} pr√≥bek\")\n",
    "\n",
    "# LabelEncoder dla targetu\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n‚úÖ LabelEncoder:\")\n",
    "print(f\"   Klasy: {label_encoder.classes_}\")\n",
    "print(f\"   Kodowanie: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# StandardScaler dla cech\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "print(f\"\\n‚úÖ StandardScaler:\")\n",
    "print(f\"   Cechy przeskalowane: {X_scaled.shape[1]}\")\n",
    "print(f\"   Przyk≈Çadowe warto≈õci (pierwsze 5 cech, pierwsze 3 pr√≥bki):\")\n",
    "print(X_scaled.iloc[:3, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 5: PODZIA≈Å TRAIN/TEST - SUBJECT-WISE SPLIT\n",
    "\n",
    "‚ö†Ô∏è **WA≈ªNE**: Ca≈Çe dane jednej osoby trafiajƒÖ albo do train, albo do test. Nie dzielimy okien z tej samej osoby miƒôdzy train i test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: X_scaled, y_encoded, groups\n",
      "   Najpierw uruchom KROK 3 i KROK 4!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "BrakujƒÖce zmienne: X_scaled, y_encoded, groups",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Najpierw uruchom KROK 3 i KROK 4!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKROK 5: PODZIA≈Å TRAIN/TEST - SUBJECT-WISE SPLIT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: BrakujƒÖce zmienne: X_scaled, y_encoded, groups"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 5: PODZIA≈Å TRAIN/TEST - SUBJECT-WISE SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá zmiennych\n",
    "required_vars = ['X_scaled', 'y_encoded', 'groups']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "    print(\"   Najpierw uruchom KROK 3 i KROK 4!\")\n",
    "    raise NameError(f\"BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 5: PODZIA≈Å TRAIN/TEST - SUBJECT-WISE SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚ö†Ô∏è WA≈ªNE: Ca≈Çe dane jednej osoby trafiajƒÖ albo do train, albo do test\")\n",
    "print(\"‚ö†Ô∏è WA≈ªNE: Nie dzielimy okien z tej samej osoby miƒôdzy train i test\")\n",
    "\n",
    "# Sprawd≈∫ d≈Çugo≈õci i konwertuj groups na numpy array\n",
    "print(f\"\\nüìä Weryfikacja danych przed split:\")\n",
    "print(f\"   X_scaled shape: {X_scaled.shape}\")\n",
    "print(f\"   y_encoded length: {len(y_encoded)}\")\n",
    "print(f\"   groups length: {len(groups)}\")\n",
    "\n",
    "if len(groups) != len(X_scaled) or len(groups) != len(y_encoded):\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: D≈Çugo≈õci siƒô nie zgadzajƒÖ!\")\n",
    "    print(f\"   X_scaled: {len(X_scaled)}, y_encoded: {len(y_encoded)}, groups: {len(groups)}\")\n",
    "    raise ValueError(\"D≈Çugo≈õci X_scaled, y_encoded i groups muszƒÖ byƒá identyczne!\")\n",
    "\n",
    "# Konwertuj groups na numpy array (wymagane przez GroupShuffleSplit)\n",
    "groups_array = np.array(groups)\n",
    "print(f\"   ‚úÖ groups skonwertowany na numpy array: {groups_array.shape}\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas per subject (przed split)\n",
    "print(f\"\\nüìä ROZK≈ÅAD KLAS PER SUBJECT (przed split):\")\n",
    "print(\"-\" * 80)\n",
    "unique_subjects = np.unique(groups_array)\n",
    "subject_class_dist = {}\n",
    "for subject in unique_subjects:\n",
    "    subject_mask = groups_array == subject\n",
    "    subject_labels = y_encoded[subject_mask]\n",
    "    subject_dist = pd.Series(label_encoder.inverse_transform(subject_labels)).value_counts()\n",
    "    subject_class_dist[subject] = subject_dist\n",
    "    print(f\"\\n  {subject}:\")\n",
    "    for label in subject_dist.index:\n",
    "        print(f\"    {label:12s}: {subject_dist[label]:4d} pr√≥bek\")\n",
    "\n",
    "# STRATIFIED SUBJECT-WISE SPLIT: Upewnij siƒô, ≈ºe w train i test sƒÖ obecne obie klasy\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STRATIFIED SUBJECT-WISE SPLIT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚ö†Ô∏è WA≈ªNE: Upewniamy siƒô, ≈ºe w train i test sƒÖ obecne obie klasy!\")\n",
    "print(\"‚ö†Ô∏è WA≈ªNE: W train muszƒÖ byƒá co najmniej 2 klasy (wymagane dla SMOTE)!\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas per subject (przed split)\n",
    "print(f\"\\nüìä ROZK≈ÅAD KLAS PER SUBJECT (przed split):\")\n",
    "print(\"-\" * 80)\n",
    "unique_subjects = np.unique(groups_array)\n",
    "subject_class_dist = {}\n",
    "for subject in unique_subjects:\n",
    "    subject_mask = groups_array == subject\n",
    "    subject_labels = y_encoded[subject_mask]\n",
    "    subject_dist = pd.Series(label_encoder.inverse_transform(subject_labels)).value_counts()\n",
    "    subject_class_dist[subject] = subject_dist\n",
    "    print(f\"\\n  {subject}:\")\n",
    "    for label in subject_dist.index:\n",
    "        print(f\"    {label:12s}: {subject_dist[label]:4d} pr√≥bek\")\n",
    "\n",
    "# U≈ºyj GroupShuffleSplit z wieloma pr√≥bami, aby znale≈∫ƒá podzia≈Ç z obiema klasami w train i test\n",
    "gss = GroupShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "\n",
    "best_train_idx = None\n",
    "best_test_idx = None\n",
    "best_score = -np.inf\n",
    "best_train_subjects = None\n",
    "best_test_subjects = None\n",
    "\n",
    "print(f\"\\nüîç Szukam najlepszego podzia≈Çu (testujƒô {gss.n_splits} r√≥≈ºnych podzia≈Ç√≥w)...\")\n",
    "\n",
    "for train_idx, test_idx in gss.split(X_scaled, y_encoded, groups=groups_array):\n",
    "    # Sprawd≈∫ czy w train i test sƒÖ obecne obie klasy\n",
    "    train_classes = np.unique(y_encoded[train_idx])\n",
    "    test_classes = np.unique(y_encoded[test_idx])\n",
    "    \n",
    "    # Wszystkie klasy muszƒÖ byƒá w train (wymagane dla SMOTE)\n",
    "    # W test powinna byƒá co najmniej jedna klasa (ale najlepiej obie)\n",
    "    if len(train_classes) >= 2:\n",
    "        # Sprawd≈∫ subjecty w train i test\n",
    "        train_subjects_set = set(groups_array[train_idx])\n",
    "        test_subjects_set = set(groups_array[test_idx])\n",
    "        \n",
    "        # Oblicz \"score\" - preferuj podzia≈Çy z obiema klasami w test i wiƒôcej subject√≥w\n",
    "        score = len(train_classes) * 10 + len(test_classes) * 5 + len(train_subjects_set) + len(test_subjects_set)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_train_idx = train_idx\n",
    "            best_test_idx = test_idx\n",
    "            best_train_subjects = train_subjects_set\n",
    "            best_test_subjects = test_subjects_set\n",
    "\n",
    "# Je≈õli nie znaleziono podzia≈Çu z obiema klasami w train, rzuƒá b≈ÇƒÖd\n",
    "if best_train_idx is None:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: Nie znaleziono podzia≈Çu z co najmniej 2 klasami w train!\")\n",
    "    print(f\"   To oznacza, ≈ºe dane sƒÖ zbyt niezbalansowane lub subjecty majƒÖ tylko jednƒÖ klasƒô.\")\n",
    "    print(f\"   Sprawd≈∫ rozk≈Çad klas per subject powy≈ºej.\")\n",
    "    raise ValueError(\"Nie mo≈ºna utworzyƒá podzia≈Çu z co najmniej 2 klasami w train - SMOTE nie bƒôdzie dzia≈Çaƒá!\")\n",
    "\n",
    "train_idx = best_train_idx\n",
    "test_idx = best_test_idx\n",
    "\n",
    "print(f\"\\n‚úÖ Znaleziono najlepszy podzia≈Ç:\")\n",
    "print(f\"   Score: {best_score}\")\n",
    "print(f\"   Train subjects: {sorted(best_train_subjects)} ({len(best_train_subjects)} subject√≥w)\")\n",
    "print(f\"   Test subjects: {sorted(best_test_subjects)} ({len(best_test_subjects)} subject√≥w)\")\n",
    "\n",
    "X_train = X_scaled.iloc[train_idx].copy()\n",
    "X_test = X_scaled.iloc[test_idx].copy()\n",
    "y_train = y_encoded[train_idx]\n",
    "y_test = y_encoded[test_idx]\n",
    "groups_train = [groups[i] for i in train_idx]\n",
    "groups_test = [groups[i] for i in test_idx]\n",
    "\n",
    "# Sprawd≈∫ kt√≥re subjecty trafi≈Çy do train/test\n",
    "train_subjects = set(groups_train)\n",
    "test_subjects = set(groups_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Podzia≈Ç subject-wise:\")\n",
    "print(f\"   Train subjects: {sorted(train_subjects)} ({len(train_subjects)} subject√≥w)\")\n",
    "print(f\"   Test subjects: {sorted(test_subjects)} ({len(test_subjects)} subject√≥w)\")\n",
    "\n",
    "# Weryfikacja: train i test subjects sƒÖ roz≈ÇƒÖczne\n",
    "if train_subjects & test_subjects:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: Train i test subjects siƒô nak≈ÇadajƒÖ!\")\n",
    "    raise ValueError(\"Subject-wise split nie dzia≈Ça poprawnie!\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Train i test subjects sƒÖ roz≈ÇƒÖczne - OK\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas w train i test\n",
    "print(f\"\\nüìä Rozk≈Çad klas w TRAIN:\")\n",
    "train_dist = pd.Series(label_encoder.inverse_transform(y_train)).value_counts()\n",
    "for label in train_dist.index:\n",
    "    count = train_dist[label]\n",
    "    pct = (count / len(y_train) * 100) if len(y_train) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Rozk≈Çad klas w TEST:\")\n",
    "test_dist = pd.Series(label_encoder.inverse_transform(y_test)).value_counts()\n",
    "for label in test_dist.index:\n",
    "    count = test_dist[label]\n",
    "    pct = (count / len(y_test) * 100) if len(y_test) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "# Sprawdzenie liczby pr√≥bek per klasƒô (u≈ºywajƒÖc kodu u≈ºytkownika)\n",
    "print(f\"\\nüìä SZCZEG√ì≈ÅOWE SPRAWDZENIE LICZBY PR√ìBEK PER KLASƒò:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"TRAIN:\")\n",
    "for label in np.unique(y_train):\n",
    "    label_name = label_encoder.inverse_transform([label])[0]\n",
    "    count = np.sum(y_train == label)\n",
    "    print(f\"   {label_name:12s} (kod {label}): {count:4d} pr√≥bek\")\n",
    "\n",
    "print(\"\\nTEST:\")\n",
    "for label in np.unique(y_test):\n",
    "    label_name = label_encoder.inverse_transform([label])[0]\n",
    "    count = np.sum(y_test == label)\n",
    "    print(f\"   {label_name:12s} (kod {label}): {count:4d} pr√≥bek\")\n",
    "\n",
    "# Weryfikacja: czy w train sƒÖ co najmniej 2 klasy (wymagane dla SMOTE)\n",
    "train_unique_classes = np.unique(y_train)\n",
    "test_unique_classes = np.unique(y_test)\n",
    "\n",
    "print(f\"\\n‚úÖ WERYFIKACJA KLAS:\")\n",
    "print(f\"   Train: {len(train_unique_classes)} klas - {label_encoder.inverse_transform(train_unique_classes)}\")\n",
    "print(f\"   Test: {len(test_unique_classes)} klas - {label_encoder.inverse_transform(test_unique_classes)}\")\n",
    "\n",
    "if len(train_unique_classes) < 2:\n",
    "    print(f\"\\n‚ö†Ô∏è OSTRZE≈ªENIE: Tylko {len(train_unique_classes)} klas w train!\")\n",
    "    print(f\"   SMOTE wymaga co najmniej 2 klas. Balansowanie mo≈ºe nie dzia≈Çaƒá.\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Train ma co najmniej 2 klasy - SMOTE mo≈ºe dzia≈Çaƒá\")\n",
    "\n",
    "print(f\"\\n‚úÖ Podzia≈Ç zako≈Ñczony:\")\n",
    "print(f\"   Train: {len(X_train)} pr√≥bek\")\n",
    "print(f\"   Test: {len(X_test)} pr√≥bek\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 6: BALANSOWANIE DANYCH W TRENINGU (SMOTE)\n",
    "\n",
    "Zastosujemy SMOTE **TYLKO na train**. Test pozostaje niezmieniony.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KROK 6: BALANSOWANIE DANYCH W TRENINGU (SMOTE)\n",
      "================================================================================\n",
      "‚ö†Ô∏è WA≈ªNE: SMOTE TYLKO na train, test pozostaje niezmieniony!\n",
      "\n",
      "üìä Rozk≈Çad klas PRZED SMOTE (train):\n",
      "   baseline    :   54 pr√≥bek ( 65.9%)\n",
      "   emotion     :   28 pr√≥bek ( 34.1%)\n",
      "\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: imbalanced-learn nie jest dostƒôpny!\n",
      "   Zainstaluj: pip install imbalanced-learn\n",
      "\n",
      "üìä Rozk≈Çad klas PO SMOTE (train):\n",
      "   baseline    :   54 pr√≥bek ( 65.9%)\n",
      "   emotion     :   28 pr√≥bek ( 34.1%)\n",
      "\n",
      "üìä Balance ratio: 0.5185\n",
      "   ‚ö†Ô∏è Klasy sƒÖ czƒô≈õciowo zbalansowane\n",
      "\n",
      "üìä Rozk≈Çad klas w TEST (niezmieniony, bez SMOTE):\n",
      "   baseline    :   26 pr√≥bek ( 63.4%)\n",
      "   emotion     :   15 pr√≥bek ( 36.6%)\n",
      "\n",
      "‚úÖ Test pozostaje niezbalansowany - to jest poprawne dla realnej ewaluacji!\n",
      "\n",
      "üìä PODSUMOWANIE:\n",
      "   Train przed SMOTE: 82 pr√≥bek, 2 klas\n",
      "   Train po SMOTE: 82 pr√≥bek, 2 klas\n",
      "   Test: 41 pr√≥bek, 2 klas\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 6: BALANSOWANIE DANYCH W TRENINGU (SMOTE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 6: BALANSOWANIE DANYCH W TRENINGU (SMOTE)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚ö†Ô∏è WA≈ªNE: SMOTE TYLKO na train, test pozostaje niezmieniony!\")\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá zmiennych\n",
    "required_vars = ['X_train', 'y_train', 'y_test', 'label_encoder']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "    print(\"   Najpierw uruchom KROK 4 i KROK 5!\")\n",
    "    raise NameError(f\"BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas przed SMOTE\n",
    "print(f\"\\nüìä Rozk≈Çad klas PRZED SMOTE (train):\")\n",
    "train_dist_before = pd.Series(label_encoder.inverse_transform(y_train)).value_counts()\n",
    "for label in train_dist_before.index:\n",
    "    count = train_dist_before[label]\n",
    "    pct = (count / len(y_train) * 100) if len(y_train) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "# Sprawd≈∫ czy mamy wiƒôcej ni≈º jednƒÖ klasƒô (SMOTE wymaga co najmniej 2 klas)\n",
    "unique_classes = np.unique(y_train)\n",
    "n_classes = len(unique_classes)\n",
    "\n",
    "if n_classes < 2:\n",
    "    print(f\"\\n‚ö†Ô∏è OSTRZE≈ªENIE: Tylko {n_classes} klas w train - SMOTE nie mo≈ºe dzia≈Çaƒá!\")\n",
    "    print(f\"   Klasy w train: {label_encoder.inverse_transform(unique_classes)}\")\n",
    "    print(f\"   SMOTE wymaga co najmniej 2 klas. U≈ºywam danych bez balansowania.\")\n",
    "    X_train_bal = X_train.copy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    y_train_bal = y_train.copy()\n",
    "elif not IMBLEARN_AVAILABLE:\n",
    "    print(\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: imbalanced-learn nie jest dostƒôpny!\")\n",
    "    print(\"   Zainstaluj: pip install imbalanced-learn\")\n",
    "    X_train_bal = X_train.copy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    y_train_bal = y_train.copy()\n",
    "else:\n",
    "    # Konwertuj X_train na numpy array je≈õli jest DataFrame\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train_array = X_train.values\n",
    "    else:\n",
    "        X_train_array = X_train\n",
    "    \n",
    "    # SMOTE wymaga co najmniej 2 klas - to ju≈º sprawdzili≈õmy wcze≈õniej\n",
    "    if n_classes >= 2:\n",
    "        # Zastosuj SMOTE\n",
    "        print(f\"\\nüîß Wykonujƒô SMOTE...\")\n",
    "        print(f\"   Liczba klas: {n_classes}\")\n",
    "        print(f\"   Klasy: {label_encoder.inverse_transform(unique_classes)}\")\n",
    "        \n",
    "        # Upewnij siƒô, ≈ºe X_train jest numpy array (SMOTE wymaga numpy array)\n",
    "        X_train_for_smote = X_train_array\n",
    "        \n",
    "        try:\n",
    "            # U≈ºyj prostego SMOTE (zgodnie z przyk≈Çadem u≈ºytkownika)\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_bal_array, y_train_bal = smote.fit_resample(X_train_for_smote, y_train)\n",
    "            \n",
    "            print(f\"‚úÖ SMOTE.fit_resample() wykonany pomy≈õlnie!\")\n",
    "            \n",
    "            # Konwertuj z powrotem na DataFrame je≈õli X_train by≈Ç DataFrame\n",
    "            if isinstance(X_train, pd.DataFrame):\n",
    "                X_train_bal = pd.DataFrame(X_train_bal_array, columns=X_train.columns)\n",
    "            else:\n",
    "                X_train_bal = X_train_bal_array\n",
    "            \n",
    "            # Weryfikacja SMOTE - sprawd≈∫ rozk≈Çad klas po SMOTE\n",
    "            print(f\"\\nüìä WERYFIKACJA SMOTE:\")\n",
    "            print(f\"   Train przed SMOTE: {len(X_train)} pr√≥bek\")\n",
    "            print(f\"   Train po SMOTE: {len(X_train_bal)} pr√≥bek\")\n",
    "            \n",
    "            # Sprawd≈∫ rozk≈Çad klas po SMOTE (u≈ºywajƒÖc kodu u≈ºytkownika)\n",
    "            print(f\"\\nüìä ROZK≈ÅAD KLAS PO SMOTE (train) - SZCZEG√ì≈ÅOWO:\")\n",
    "            print(\"-\" * 80)\n",
    "            for label in np.unique(y_train_bal):\n",
    "                label_name = label_encoder.inverse_transform([label])[0]\n",
    "                count = np.sum(y_train_bal == label)\n",
    "                pct = (count / len(y_train_bal) * 100) if len(y_train_bal) > 0 else 0\n",
    "                print(f\"   {label_name:12s} (kod {label}): {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "            \n",
    "            # Weryfikacja balansu\n",
    "            unique_labels_after = np.unique(y_train_bal)\n",
    "            if len(unique_labels_after) == 2:\n",
    "                counts = [np.sum(y_train_bal == label) for label in unique_labels_after]\n",
    "                balance_ratio = min(counts) / max(counts)\n",
    "                print(f\"\\nüìä Balance ratio: {balance_ratio:.4f}\")\n",
    "                if balance_ratio >= 0.95:\n",
    "                    print(f\"   ‚úÖ‚úÖ‚úÖ IDEALNY BALANS! Klasy sƒÖ zbalansowane (balance ratio >= 0.95)!\")\n",
    "                elif balance_ratio >= 0.8:\n",
    "                    print(f\"   ‚úÖ Dobry balans (balance ratio >= 0.8)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Czƒô≈õciowy balans (balance ratio < 0.8)\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ SMOTE zako≈Ñczony pomy≈õlnie!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå B≈ÅƒÑD podczas SMOTE: {e}\")\n",
    "            print(f\"   U≈ºywam danych bez balansowania.\")\n",
    "            X_train_bal = X_train.copy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "            y_train_bal = y_train.copy()\n",
    "    \n",
    "# Sprawd≈∫ rozk≈Çad klas po SMOTE (lub bez SMOTE je≈õli nie by≈Ço mo≈ºliwe)\n",
    "print(f\"\\nüìä Rozk≈Çad klas PO SMOTE (train):\")\n",
    "train_dist_after = pd.Series(label_encoder.inverse_transform(y_train_bal)).value_counts()\n",
    "for label in train_dist_after.index:\n",
    "    count = train_dist_after[label]\n",
    "    pct = (count / len(y_train_bal) * 100) if len(y_train_bal) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "# Sprawd≈∫ balance ratio (tylko je≈õli mamy wiƒôcej ni≈º jednƒÖ klasƒô)\n",
    "if len(train_dist_after) >= 2:\n",
    "    balance_ratio = min(train_dist_after.values) / max(train_dist_after.values)\n",
    "    print(f\"\\nüìä Balance ratio: {balance_ratio:.4f}\")\n",
    "    \n",
    "    baseline_count = train_dist_after.get('baseline', 0)\n",
    "    emotion_count = train_dist_after.get('emotion', 0) if 'emotion' in train_dist_after.index else 0\n",
    "    stress_count = train_dist_after.get('stress', 0) if 'stress' in train_dist_after.index else 0\n",
    "    amusement_count = train_dist_after.get('amusement', 0) if 'amusement' in train_dist_after.index else 0\n",
    "    \n",
    "    if baseline_count > 0 and (emotion_count > 0 or stress_count > 0 or amusement_count > 0):\n",
    "        minority_count = emotion_count + stress_count + amusement_count\n",
    "        if baseline_count == minority_count:\n",
    "            print(f\"   ‚úÖ‚úÖ‚úÖ IDEALNY BALANS! baseline i emotion/stress/amusement majƒÖ tyle samo pr√≥bek!\")\n",
    "        elif balance_ratio >= 0.95:\n",
    "            print(f\"   ‚úÖ‚úÖ‚úÖ KLASY SƒÑ ZBALANSOWANE (balance ratio >= 0.95)!\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Klasy sƒÖ czƒô≈õciowo zbalansowane\")\n",
    "elif len(train_dist_after) == 1:\n",
    "    print(f\"\\n‚ö†Ô∏è OSTRZE≈ªENIE: Tylko jedna klasa w train po SMOTE - balansowanie nie by≈Ço mo≈ºliwe\")\n",
    "\n",
    "# Weryfikacja: test pozostaje niezmieniony\n",
    "print(f\"\\nüìä Rozk≈Çad klas w TEST (niezmieniony, bez SMOTE):\")\n",
    "test_dist_unchanged = pd.Series(label_encoder.inverse_transform(y_test)).value_counts()\n",
    "for label in test_dist_unchanged.index:\n",
    "    count = test_dist_unchanged[label]\n",
    "    pct = (count / len(y_test) * 100) if len(y_test) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} pr√≥bek ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Test pozostaje niezbalansowany - to jest poprawne dla realnej ewaluacji!\")\n",
    "\n",
    "# Weryfikacja ko≈Ñcowa\n",
    "print(f\"\\nüìä PODSUMOWANIE:\")\n",
    "print(f\"   Train przed SMOTE: {len(X_train)} pr√≥bek, {len(np.unique(y_train))} klas\")\n",
    "print(f\"   Train po SMOTE: {len(X_train_bal)} pr√≥bek, {len(np.unique(y_train_bal))} klas\")\n",
    "print(f\"   Test: {len(y_test)} pr√≥bek, {len(np.unique(y_test))} klas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 7: PRZYGOTOWANIE DANYCH DLA TIME SERIES\n",
    "\n",
    "Przekszta≈Çcamy dane z okien w sekwencje czasowe dla modelu LSTM/GRU.\n",
    "\n",
    "Ka≈ºda sekwencja sk≈Çada siƒô z N kolejnych okien (timesteps), gdzie ka≈ºde okno to wektor cech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KROK 7: PRZYGOTOWANIE DANYCH DLA TIME SERIES\n",
      "================================================================================\n",
      "\n",
      "üîç Sprawdzam dostƒôpno≈õƒá TensorFlow...\n",
      "   TENSORFLOW_AVAILABLE = False\n",
      "\n",
      "================================================================================\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: TensorFlow/Keras nie jest dostƒôpny!\n",
      "================================================================================\n",
      "\n",
      "üìã INSTRUKCJA:\n",
      "   1. Wr√≥ƒá do kom√≥rki KROK 1 (Cell 1)\n",
      "   2. Znajd≈∫ liniƒô: SKIP_TENSORFLOW = True\n",
      "   3. Zmie≈Ñ na: SKIP_TENSORFLOW = False\n",
      "   4. Uruchom ponownie kom√≥rkƒô KROK 1\n",
      "\n",
      "   Je≈õli TensorFlow powoduje crash kernela:\n",
      "   - Zainstaluj TensorFlow: pip install tensorflow\n",
      "   - Lub u≈ºyj TensorFlow CPU: pip install tensorflow-cpu\n",
      "   - Sprawd≈∫ logi Jupytera dla szczeg√≥≈Ç√≥w b≈Çƒôdu\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "TensorFlow/Keras nie jest dostƒôpny - zmie≈Ñ SKIP_TENSORFLOW = False w KROK 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Sprawd≈∫ logi Jupytera dla szczeg√≥≈Ç√≥w b≈Çƒôdu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow/Keras nie jest dostƒôpny - zmie≈Ñ SKIP_TENSORFLOW = False w KROK 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Parametry sekwencji\u001b[39;00m\n\u001b[1;32m     45\u001b[0m SEQUENCE_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Liczba kolejnych okien w sekwencji (timesteps)\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: TensorFlow/Keras nie jest dostƒôpny - zmie≈Ñ SKIP_TENSORFLOW = False w KROK 1"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 7: PRZYGOTOWANIE DANYCH DLA TIME SERIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 7: PRZYGOTOWANIE DANYCH DLA TIME SERIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá zmiennych\n",
    "required_vars = ['X_train_bal', 'y_train_bal', 'X_test', 'y_test', 'groups_train', 'groups_test', 'label_encoder']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "    print(\"   Najpierw uruchom KROKI 2-6 w kolejno≈õci!\")\n",
    "    raise NameError(f\"BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá TensorFlow\n",
    "print(f\"\\nüîç Sprawdzam dostƒôpno≈õƒá TensorFlow...\")\n",
    "try:\n",
    "    tf_check = TENSORFLOW_AVAILABLE\n",
    "    print(f\"   TENSORFLOW_AVAILABLE = {tf_check}\")\n",
    "except NameError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(f\"   ‚ö†Ô∏è TENSORFLOW_AVAILABLE nie jest zdefiniowane - ustawiam na False\")\n",
    "    print(f\"   üí° Uruchom ponownie KROK 1 po zmianie SKIP_TENSORFLOW = False!\")\n",
    "\n",
    "if not TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå‚ùå‚ùå B≈ÅƒÑD: TensorFlow/Keras nie jest dostƒôpny!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìã INSTRUKCJA:\")\n",
    "    print(\"   1. Wr√≥ƒá do kom√≥rki KROK 1 (Cell 1)\")\n",
    "    print(\"   2. Znajd≈∫ liniƒô: SKIP_TENSORFLOW = True\")\n",
    "    print(\"   3. Zmie≈Ñ na: SKIP_TENSORFLOW = False\")\n",
    "    print(\"   4. Uruchom ponownie kom√≥rkƒô KROK 1\")\n",
    "    print(\"\\n   Je≈õli TensorFlow powoduje crash kernela:\")\n",
    "    print(\"   - Zainstaluj TensorFlow: pip install tensorflow\")\n",
    "    print(\"   - Lub u≈ºyj TensorFlow CPU: pip install tensorflow-cpu\")\n",
    "    print(\"   - Sprawd≈∫ logi Jupytera dla szczeg√≥≈Ç√≥w b≈Çƒôdu\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    raise ImportError(\"TensorFlow/Keras nie jest dostƒôpny - zmie≈Ñ SKIP_TENSORFLOW = False w KROK 1\")\n",
    "\n",
    "# Parametry sekwencji\n",
    "SEQUENCE_LENGTH = 5  # Liczba kolejnych okien w sekwencji (timesteps)\n",
    "\n",
    "print(f\"\\nüìä PARAMETRY SEKWENCJI:\")\n",
    "print(f\"   D≈Çugo≈õƒá sekwencji (timesteps): {SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Sprawd≈∫ wymiar cech\n",
    "if hasattr(X_train_bal, 'shape'):\n",
    "    n_features = X_train_bal.shape[1]\n",
    "    print(f\"   Ka≈ºde okno = wektor cech o wymiarze: {n_features}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Nie mo≈ºna okre≈õliƒá wymiaru cech\")\n",
    "\n",
    "def create_sequences(X_data, y_data, groups_data, sequence_length=5):\n",
    "    \"\"\"\n",
    "    Tworzy sekwencje czasowe z okien.\n",
    "    \n",
    "    Args:\n",
    "        X_data: DataFrame lub array z cechami (n_samples, n_features)\n",
    "        y_data: array z etykietami (n_samples,)\n",
    "        groups_data: lista z identyfikatorami subject√≥w (n_samples,)\n",
    "        sequence_length: d≈Çugo≈õƒá sekwencji (liczba okien)\n",
    "    \n",
    "    Returns:\n",
    "        X_sequences: array (n_sequences, sequence_length, n_features)\n",
    "        y_sequences: array (n_sequences,) - etykieta ostatniego okna w sekwencji\n",
    "        groups_sequences: lista (n_sequences,) - subject dla ka≈ºdej sekwencji\n",
    "    \"\"\"\n",
    "    # Konwertuj na numpy array\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_array = X_data.values\n",
    "    else:\n",
    "        X_array = np.array(X_data)\n",
    "    \n",
    "    # Konwertuj y_data na numpy array\n",
    "    if isinstance(y_data, (pd.Series, pd.DataFrame)):\n",
    "        y_array = y_data.values\n",
    "    elif isinstance(y_data, np.ndarray):\n",
    "        y_array = y_data\n",
    "    else:\n",
    "        y_array = np.array(y_data)\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    groups_sequences = []\n",
    "    \n",
    "    # Grupuj dane per subject (aby tworzyƒá sekwencje tylko z okien tego samego subjecta)\n",
    "    # Konwertuj groups_data na listƒô je≈õli to numpy array\n",
    "    if isinstance(groups_data, np.ndarray):\n",
    "        groups_list = groups_data.tolist()\n",
    "    elif isinstance(groups_data, pd.Series):\n",
    "        groups_list = groups_data.tolist()\n",
    "    else:\n",
    "        groups_list = list(groups_data)\n",
    "    \n",
    "    # Sprawd≈∫ d≈Çugo≈õci\n",
    "    if len(X_array) != len(y_array) or len(X_array) != len(groups_list):\n",
    "        raise ValueError(f\"Niezgodno≈õƒá d≈Çugo≈õci: X_data={len(X_array)}, y_data={len(y_array)}, groups_data={len(groups_list)}\")\n",
    "    \n",
    "    unique_groups = list(set(groups_list))\n",
    "    \n",
    "    for group in unique_groups:\n",
    "        # Znajd≈∫ indeksy dla tego subjecta\n",
    "        group_indices = [i for i, g in enumerate(groups_list) if g == group]\n",
    "        \n",
    "        if len(group_indices) < sequence_length:\n",
    "            # Za ma≈Ço okien dla tego subjecta - pomi≈Ñ\n",
    "            continue\n",
    "        \n",
    "        # Sortuj indeksy (aby zachowaƒá kolejno≈õƒá czasowƒÖ)\n",
    "        group_indices = sorted(group_indices)\n",
    "        \n",
    "        # Tw√≥rz sekwencje z kolejnych okien\n",
    "        for i in range(len(group_indices) - sequence_length + 1):\n",
    "            seq_indices = group_indices[i:i + sequence_length]\n",
    "            \n",
    "            # Sprawd≈∫ czy mamy wystarczajƒÖcƒÖ liczbƒô indeks√≥w\n",
    "            if len(seq_indices) < sequence_length:\n",
    "                continue\n",
    "            \n",
    "            # Konwertuj seq_indices na numpy array dla indeksowania\n",
    "            seq_indices_array = np.array(seq_indices)\n",
    "            \n",
    "            # Sprawd≈∫ czy wszystkie okna w sekwencji majƒÖ tƒô samƒÖ etykietƒô\n",
    "            # (opcjonalnie - mo≈ºemy te≈º u≈ºyƒá etykiety ostatniego okna)\n",
    "            seq_labels = y_array[seq_indices_array]\n",
    "            # U≈ºyj etykiety ostatniego okna w sekwencji\n",
    "            seq_label = seq_labels[-1]\n",
    "            \n",
    "            # Sprawd≈∫ czy wszystkie okna majƒÖ tƒô samƒÖ etykietƒô (opcjonalna walidacja)\n",
    "            if len(np.unique(seq_labels)) > 1:\n",
    "                # Sekwencja zawiera okna z r√≥≈ºnymi etykietami - mo≈ºemy jƒÖ pominƒÖƒá lub u≈ºyƒá\n",
    "                # Dla uproszczenia u≈ºywamy etykiety ostatniego okna\n",
    "                pass\n",
    "            \n",
    "            # Utw√≥rz sekwencjƒô cech\n",
    "            X_seq = X_array[seq_indices_array]  # (sequence_length, n_features)\n",
    "            X_sequences.append(X_seq)\n",
    "            y_sequences.append(seq_label)\n",
    "            groups_sequences.append(group)\n",
    "    \n",
    "    if len(X_sequences) == 0:\n",
    "        raise ValueError(\"Nie utworzono ≈ºadnych sekwencji! Sprawd≈∫ dane wej≈õciowe i sequence_length.\")\n",
    "    \n",
    "    return np.array(X_sequences), np.array(y_sequences), groups_sequences\n",
    "\n",
    "# Tworzenie sekwencji dla train\n",
    "print(f\"\\nüîß Tworzenie sekwencji dla TRAIN...\")\n",
    "print(f\"   Sprawdzam dane wej≈õciowe:\")\n",
    "print(f\"   - X_train_bal type: {type(X_train_bal)}, shape: {X_train_bal.shape if hasattr(X_train_bal, 'shape') else 'N/A'}\")\n",
    "print(f\"   - y_train_bal type: {type(y_train_bal)}, length: {len(y_train_bal) if hasattr(y_train_bal, '__len__') else 'N/A'}\")\n",
    "print(f\"   - groups_train type: {type(groups_train)}, length: {len(groups_train) if hasattr(groups_train, '__len__') else 'N/A'}\")\n",
    "\n",
    "try:\n",
    "    X_train_seq, y_train_seq, groups_train_seq = create_sequences(\n",
    "        X_train_bal, y_train_bal, groups_train, SEQUENCE_LENGTH\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD podczas tworzenia sekwencji TRAIN:\")\n",
    "    print(f\"   B≈ÇƒÖd: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(f\"‚úÖ Sekwencje TRAIN utworzone:\")\n",
    "print(f\"   Kszta≈Çt X_train_seq: {X_train_seq.shape} (n_sequences, timesteps, n_features)\")\n",
    "print(f\"   Kszta≈Çt y_train_seq: {y_train_seq.shape}\")\n",
    "\n",
    "# Tworzenie sekwencji dla test\n",
    "print(f\"\\nüîß Tworzenie sekwencji dla TEST...\")\n",
    "print(f\"   Sprawdzam dane wej≈õciowe:\")\n",
    "print(f\"   - X_test type: {type(X_test)}, shape: {X_test.shape if hasattr(X_test, 'shape') else 'N/A'}\")\n",
    "print(f\"   - y_test type: {type(y_test)}, length: {len(y_test) if hasattr(y_test, '__len__') else 'N/A'}\")\n",
    "print(f\"   - groups_test type: {type(groups_test)}, length: {len(groups_test) if hasattr(groups_test, '__len__') else 'N/A'}\")\n",
    "\n",
    "try:\n",
    "    X_test_seq, y_test_seq, groups_test_seq = create_sequences(\n",
    "        X_test, y_test, groups_test, SEQUENCE_LENGTH\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD podczas tworzenia sekwencji TEST:\")\n",
    "    print(f\"   B≈ÇƒÖd: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(f\"‚úÖ Sekwencje TEST utworzone:\")\n",
    "print(f\"   Kszta≈Çt X_test_seq: {X_test_seq.shape} (n_sequences, timesteps, n_features)\")\n",
    "print(f\"   Kszta≈Çt y_test_seq: {y_test_seq.shape}\")\n",
    "\n",
    "# Sprawd≈∫ rozk≈Çad klas w sekwencjach\n",
    "print(f\"\\nüìä ROZK≈ÅAD KLAS W SEKWENCJACH:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"TRAIN:\")\n",
    "train_seq_dist = pd.Series(label_encoder.inverse_transform(y_train_seq)).value_counts()\n",
    "for label in train_seq_dist.index:\n",
    "    count = train_seq_dist[label]\n",
    "    pct = (count / len(y_train_seq) * 100) if len(y_train_seq) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} sekwencji ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nTEST:\")\n",
    "test_seq_dist = pd.Series(label_encoder.inverse_transform(y_test_seq)).value_counts()\n",
    "for label in test_seq_dist.index:\n",
    "    count = test_seq_dist[label]\n",
    "    pct = (count / len(y_test_seq) * 100) if len(y_test_seq) > 0 else 0\n",
    "    print(f\"   {label:12s}: {count:4d} sekwencji ({pct:5.1f}%)\")\n",
    "\n",
    "# Konwersja etykiet na kategorie (one-hot encoding dla Keras)\n",
    "print(f\"\\nüîß Konwersja etykiet na kategorie (one-hot encoding)...\")\n",
    "n_classes = len(label_encoder.classes_)\n",
    "print(f\"   Liczba klas: {n_classes}\")\n",
    "print(f\"   Klasy: {label_encoder.classes_}\")\n",
    "\n",
    "try:\n",
    "    y_train_seq_categorical = to_categorical(y_train_seq, num_classes=n_classes)\n",
    "    y_test_seq_categorical = to_categorical(y_test_seq, num_classes=n_classes)\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD podczas konwersji etykiet:\")\n",
    "    print(f\"   B≈ÇƒÖd: {type(e).__name__}: {e}\")\n",
    "    print(f\"   y_train_seq unique values: {np.unique(y_train_seq) if len(y_train_seq) > 0 else 'empty'}\")\n",
    "    print(f\"   y_test_seq unique values: {np.unique(y_test_seq) if len(y_test_seq) > 0 else 'empty'}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(f\"\\n‚úÖ Etykiety przekonwertowane na kategorie:\")\n",
    "print(f\"   Kszta≈Çt y_train_seq_categorical: {y_train_seq_categorical.shape}\")\n",
    "print(f\"   Kszta≈Çt y_test_seq_categorical: {y_test_seq_categorical.shape}\")\n",
    "print(f\"   Liczba klas: {n_classes}\")\n",
    "\n",
    "print(f\"\\n‚úÖ PRZYGOTOWANIE DANYCH DLA TIME SERIES ZAKO≈ÉCZONE!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 8: TRENOWANIE MODELU TIME SERIES (LSTM/GRU)\n",
    "\n",
    "Trenujemy model LSTM/GRU do klasyfikacji sekwencji czasowych.\n",
    "\n",
    "Model uczy siƒô zale≈ºno≈õci temporalnych miƒôdzy kolejnymi oknami.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KROK 8: TRENOWANIE MODELU TIME SERIES (LSTM/GRU)\n",
      "================================================================================\n",
      "\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: X_train_seq, y_train_seq_categorical, X_test_seq, y_test_seq_categorical\n",
      "   Najpierw uruchom KROK 7!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "BrakujƒÖce zmienne: X_train_seq, y_train_seq_categorical, X_test_seq, y_test_seq_categorical",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Najpierw uruchom KROK 7!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Sprawd≈∫ dostƒôpno≈õƒá TensorFlow\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: BrakujƒÖce zmienne: X_train_seq, y_train_seq_categorical, X_test_seq, y_test_seq_categorical"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 8: TRENOWANIE MODELU TIME SERIES (LSTM/GRU)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 8: TRENOWANIE MODELU TIME SERIES (LSTM/GRU)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá zmiennych\n",
    "required_vars = ['X_train_seq', 'y_train_seq_categorical', 'X_test_seq', 'y_test_seq_categorical']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "    print(\"   Najpierw uruchom KROK 7!\")\n",
    "    raise NameError(f\"BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá TensorFlow\n",
    "try:\n",
    "    TENSORFLOW_AVAILABLE\n",
    "except NameError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "if not TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå‚ùå‚ùå B≈ÅƒÑD: TensorFlow/Keras nie jest dostƒôpny!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìã INSTRUKCJA:\")\n",
    "    print(\"   1. Wr√≥ƒá do kom√≥rki KROK 1 (Cell 1)\")\n",
    "    print(\"   2. Znajd≈∫ liniƒô: SKIP_TENSORFLOW = True\")\n",
    "    print(\"   3. Zmie≈Ñ na: SKIP_TENSORFLOW = False\")\n",
    "    print(\"   4. Uruchom ponownie kom√≥rkƒô KROK 1\")\n",
    "    print(\"\\n   Je≈õli TensorFlow powoduje crash kernela:\")\n",
    "    print(\"   - Zainstaluj TensorFlow: pip install tensorflow\")\n",
    "    print(\"   - Lub u≈ºyj TensorFlow CPU: pip install tensorflow-cpu\")\n",
    "    print(\"   - Sprawd≈∫ logi Jupytera dla szczeg√≥≈Ç√≥w b≈Çƒôdu\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    raise ImportError(\"TensorFlow/Keras nie jest dostƒôpny - zmie≈Ñ SKIP_TENSORFLOW = False w KROK 1\")\n",
    "\n",
    "# Parametry modelu\n",
    "SEQUENCE_LENGTH = X_train_seq.shape[1]\n",
    "N_FEATURES = X_train_seq.shape[2]\n",
    "N_CLASSES = y_train_seq_categorical.shape[1]\n",
    "\n",
    "print(f\"\\nüìä PARAMETRY MODELU:\")\n",
    "print(f\"   D≈Çugo≈õƒá sekwencji (timesteps): {SEQUENCE_LENGTH}\")\n",
    "print(f\"   Liczba cech per timestep: {N_FEATURES}\")\n",
    "print(f\"   Liczba klas: {N_CLASSES}\")\n",
    "\n",
    "# Wyb√≥r typu modelu (LSTM lub GRU)\n",
    "MODEL_TYPE = \"LSTM\"  # Mo≈ºna zmieniƒá na \"GRU\"\n",
    "\n",
    "print(f\"\\nüîß Budowanie modelu {MODEL_TYPE}...\")\n",
    "\n",
    "# Budowa modelu\n",
    "model = Sequential()\n",
    "\n",
    "# Warstwa LSTM/GRU\n",
    "if MODEL_TYPE == \"LSTM\":\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, N_FEATURES)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "elif MODEL_TYPE == \"GRU\":\n",
    "    model.add(GRU(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, N_FEATURES)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(GRU(32, return_sequences=False))\n",
    "else:\n",
    "    raise ValueError(f\"Nieznany typ modelu: {MODEL_TYPE}\")\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Warstwa wyj≈õciowa\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model zbudowany:\")\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Trenowanie modelu\n",
    "print(f\"\\nüîß Rozpoczynam trenowanie modelu...\")\n",
    "print(f\"   Train sequences: {len(X_train_seq)}\")\n",
    "print(f\"   Test sequences: {len(X_test_seq)}\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq_categorical,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test_seq, y_test_seq_categorical),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Trenowanie zako≈Ñczone!\")\n",
    "\n",
    "# Wizualizacja historii trenowania\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Wykres accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Wykres loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Model {MODEL_TYPE} wytrenowany pomy≈õlnie!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 9: EWALUACJA MODELU TIME SERIES\n",
    "\n",
    "Oceniamy wyniki modelu time series u≈ºywajƒÖc tych samych metryk co w oryginalnym pliku.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KROK 9: EWALUACJA MODELU TIME SERIES\n",
      "================================================================================\n",
      "\n",
      "‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: model, X_test_seq, y_test_seq, label_encoder, MODEL_TYPE\n",
      "   Najpierw uruchom KROK 8!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "BrakujƒÖce zmienne: model, X_test_seq, y_test_seq, label_encoder, MODEL_TYPE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Najpierw uruchom KROK 8!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrakujƒÖce zmienne: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Sprawd≈∫ dostƒôpno≈õƒá TensorFlow\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: BrakujƒÖce zmienne: model, X_test_seq, y_test_seq, label_encoder, MODEL_TYPE"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KROK 9: EWALUACJA MODELU TIME SERIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KROK 9: EWALUACJA MODELU TIME SERIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá zmiennych\n",
    "required_vars = ['model', 'X_test_seq', 'y_test_seq', 'label_encoder', 'MODEL_TYPE']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ùå‚ùå‚ùå B≈ÅƒÑD: BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "    print(\"   Najpierw uruchom KROK 8!\")\n",
    "    raise NameError(f\"BrakujƒÖce zmienne: {', '.join(missing_vars)}\")\n",
    "\n",
    "# Sprawd≈∫ dostƒôpno≈õƒá TensorFlow\n",
    "try:\n",
    "    TENSORFLOW_AVAILABLE\n",
    "except NameError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "if not TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå‚ùå‚ùå B≈ÅƒÑD: TensorFlow/Keras nie jest dostƒôpny!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìã INSTRUKCJA:\")\n",
    "    print(\"   1. Wr√≥ƒá do kom√≥rki KROK 1 (Cell 1)\")\n",
    "    print(\"   2. Znajd≈∫ liniƒô: SKIP_TENSORFLOW = True\")\n",
    "    print(\"   3. Zmie≈Ñ na: SKIP_TENSORFLOW = False\")\n",
    "    print(\"   4. Uruchom ponownie kom√≥rkƒô KROK 1\")\n",
    "    print(\"\\n   Je≈õli TensorFlow powoduje crash kernela:\")\n",
    "    print(\"   - Zainstaluj TensorFlow: pip install tensorflow\")\n",
    "    print(\"   - Lub u≈ºyj TensorFlow CPU: pip install tensorflow-cpu\")\n",
    "    print(\"   - Sprawd≈∫ logi Jupytera dla szczeg√≥≈Ç√≥w b≈Çƒôdu\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    raise ImportError(\"TensorFlow/Keras nie jest dostƒôpny - zmie≈Ñ SKIP_TENSORFLOW = False w KROK 1\")\n",
    "\n",
    "# Predykcje\n",
    "print(f\"\\nüîß Wykonujƒô predykcje...\")\n",
    "y_pred_proba = model.predict(X_test_seq, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(f\"‚úÖ Predykcje wykonane\")\n",
    "\n",
    "# Oblicz metryki\n",
    "accuracy = accuracy_score(y_test_seq, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test_seq, y_pred)\n",
    "macro_f1 = f1_score(y_test_seq, y_pred, average='macro')\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_seq, y_pred)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(\n",
    "    y_test_seq, \n",
    "    y_pred, \n",
    "    target_names=label_encoder.classes_,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Wyniki\n",
    "results = {\n",
    "    'model_name': f'{MODEL_TYPE}_TimeSeries',\n",
    "    'accuracy': accuracy,\n",
    "    'balanced_accuracy': balanced_acc,\n",
    "    'macro_f1': macro_f1,\n",
    "    'confusion_matrix': cm,\n",
    "    'classification_report': report,\n",
    "    'y_pred': y_pred,\n",
    "    'y_true': y_test_seq\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"WYNIKI EWALUACJI\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nüìä METRYKI GLOBALNE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "print(f\"   Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä CONFUSION MATRIX:\")\n",
    "print(\"-\" * 80)\n",
    "print(cm)\n",
    "print(f\"\\n   Klasy: {label_encoder.classes_}\")\n",
    "\n",
    "# Wizualizacja confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "ax.set_title(f'Confusion Matrix - {MODEL_TYPE} Time Series', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('True', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(\n",
    "    y_test_seq, \n",
    "    y_pred, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"\\nüìä PER-CLASS METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "for label in label_encoder.classes_:\n",
    "    label_idx = label_encoder.transform([label])[0]\n",
    "    precision = report[label]['precision']\n",
    "    recall = report[label]['recall']\n",
    "    f1 = report[label]['f1-score']\n",
    "    support = report[label]['support']\n",
    "    print(f\"\\n   {label}:\")\n",
    "    print(f\"      Precision: {precision:.4f}\")\n",
    "    print(f\"      Recall: {recall:.4f}\")\n",
    "    print(f\"      F1-Score: {f1:.4f}\")\n",
    "    print(f\"      Support: {support}\")\n",
    "\n",
    "# Por√≥wnanie z baseline (DummyClassifier)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"POR√ìWNANIE Z BASELINE (DummyClassifier)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# DummyClassifier na oryginalnych danych (nie sekwencjach)\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy.fit(X_train_seq.reshape(len(X_train_seq), -1), y_train_seq)\n",
    "y_dummy_pred = dummy.predict(X_test_seq.reshape(len(X_test_seq), -1))\n",
    "\n",
    "dummy_accuracy = accuracy_score(y_test_seq, y_dummy_pred)\n",
    "dummy_balanced_acc = balanced_accuracy_score(y_test_seq, y_dummy_pred)\n",
    "dummy_macro_f1 = f1_score(y_test_seq, y_dummy_pred, average='macro')\n",
    "\n",
    "print(f\"\\nüìä BASELINE (DummyClassifier):\")\n",
    "print(f\"   Accuracy: {dummy_accuracy:.4f}\")\n",
    "print(f\"   Balanced Accuracy: {dummy_balanced_acc:.4f}\")\n",
    "print(f\"   Macro F1: {dummy_macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä {MODEL_TYPE} TIME SERIES:\")\n",
    "print(f\"   Accuracy: {accuracy:.4f} ({'+' if accuracy > dummy_accuracy else ''}{accuracy - dummy_accuracy:.4f})\")\n",
    "print(f\"   Balanced Accuracy: {balanced_acc:.4f} ({'+' if balanced_acc > dummy_balanced_acc else ''}{balanced_acc - dummy_balanced_acc:.4f})\")\n",
    "print(f\"   Macro F1: {macro_f1:.4f} ({'+' if macro_f1 > dummy_macro_f1 else ''}{macro_f1 - dummy_macro_f1:.4f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ EWALUACJA ZAKO≈ÉCZONA!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "Ten notebook wykonuje klasyfikacjƒô emocji z danych WESAD u≈ºywajƒÖc **modelu time series (LSTM/GRU)** zamiast tradycyjnych modeli ML.\n",
    "\n",
    "### Kluczowe r√≥≈ºnice wzglƒôdem `06_klasyfikacja_emocji_smote.ipynb`:\n",
    "\n",
    "1. **Jeden model** zamiast piƒôciu (Logistic Regression, Random Forest, SVM, XGBoost, Ensemble)\n",
    "2. **Sekwencje czasowe** - dane sƒÖ przekszta≈Çcane w sekwencje kolejnych okien\n",
    "3. **Model LSTM/GRU** - uczy siƒô zale≈ºno≈õci temporalnych miƒôdzy oknami\n",
    "4. **Ta sama struktura danych** - u≈ºywa tych samych funkcji wczytywania i preprocessing\n",
    "\n",
    "### Wyniki:\n",
    "\n",
    "Model time series mo≈ºe lepiej uchwyciƒá zale≈ºno≈õci czasowe miƒôdzy kolejnymi oknami, co mo≈ºe byƒá szczeg√≥lnie przydatne dla danych fizjologicznych, gdzie stan emocjonalny zmienia siƒô stopniowo w czasie.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
